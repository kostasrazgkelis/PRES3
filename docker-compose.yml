version: '3.0'

services:
    front:
        container_name: front-end
        build:
            context: .
            dockerfile: infrastructures/front/Dockerfile
        ports:
            - "9000:9000"
        environment:
            - PORT=9000
            - DEBUG=1
            - NAME=front
        restart: always
        volumes:
            - ./src/services/front/:/src/app
            - cluster-A-volume:/src/services/app/uploaded_files_a
            - cluster-B-volume:/src/services/app/uploaded_files_b
        networks:
            - front-back-net

    cluster-a:
        container_name: cluster-a
        build:
            context: .
            args:
                NAME: cluster_a
            dockerfile: infrastructures/app/Dockerfile
        ports:
            - "9200:9200"
        environment:
            - PORT=9200
            - DEBUG=1
            - NAME=cluster_a
        restart: always
        volumes:
            - ./src/services/app/:/src/app
            - cluster-A-volume:/src/services/app/uploaded_files_a
        networks:
            - front-back-net

    cluster-b:
        container_name: cluster-b
        build: 
            context: .
            args:
                NAME: cluster_b
            dockerfile: infrastructures/app/Dockerfile
        ports:
            - "9300:9300"
        environment:
            - PORT=9300
            - DEBUG=1
            - NAME=cluster_b
        restart: always
        volumes:
            - ./src/services/app/:/src/app
            - cluster-B-volume:/src/services/app/uploaded_files_b
        networks:
            - front-back-net

#    jupyter:
#        container_name: jupyter
#        build:
#            context: ./infrastructures/jupyter
#            dockerfile: Dockerfile
#        ports:
#            - "9000:8888"
#        volumes:
#            -  shared-workspace:/opt/workspace
#        networks:
#            - spark-net
#
#    master:
#        build:
#            context: infrastructures/spark
#            dockerfile: Dockerfile
#        container_name: master
#        command: bash -c "start-master.sh && tail -F /dev/null"
#        ports:
#            - "7077:7077"
#            - "8090:8080"
#        volumes:
#            -  shared-workspace:/opt/workspace
#        networks:
#            - spark-net
#
#    worker:
#        build:
#            context: infrastructures/spark
#            dockerfile: Dockerfile
#        environment:
#            - SPARK_WORKER_CORES=3
#            - SPARK_WORKER_MEMORY=2g
#        command: bash -c "start-worker.sh spark://master:7077 && tail -F /dev/null"
#        volumes:
#            -  shared-workspace:/opt/workspace
#        networks:
#            - spark-net

volumes:    
    cluster-A-volume:
    cluster-B-volume:
    shared-workspace:
        name: "hadoop-distributed-file-system"
        driver: local

networks:
    spark-net:
    front-back-net:



