version: '3.0'

services: 
    cluster-a:
        container_name: cluster-a
        build: 
            context: ./services/data-service-1
            dockerfile: Dockerfile
        ports: 
            - "9200:9200"
        environment: 
            - PORT=9200
        volumes: 
            - cluster-A-volume:/var/lib/data


    cluster-b:
        container_name: cluster-b
        build: 
            context: ./services/data-service-2
            dockerfile: Dockerfile
        ports: 
            - "9300:9300"
        environment: 
            - PORT=9300
        volumes: 
            - cluster-B-volume:/var/lib/data
    

    jupyter:
        container_name: jupyter
        build: 
            context: ./services/jupyter
            dockerfile: Dockerfile
        ports:
            - "8888:8888"
        volumes:
            -  shared-workspace:/opt/workspace 

            
    spark-master:
        image: bde2020/spark-master:3.1.1-hadoop3.2
        container_name: spark-master-thesis
        ports:
            - "8080:8080"
            - "7077:7077"
        environment:
            - INIT_DAEMON_STEP=setup_spark
        volumes:
            -  shared-workspace:/opt/workspace  

    spark-worker-1:
        image: bde2020/spark-worker:3.1.1-hadoop3.2
        container_name: spark-worker-thesis-1
        depends_on:
            - spark-master
        ports:
            - "8081:8081"
        environment:
            - "SPARK_MASTER=spark://spark-master:7077"
        volumes:
            -  shared-workspace:/opt/workspace

volumes:    
    cluster-A-volume:
    cluster-B-volume:
    shared-workspace:
        name: "hadoop-distributed-file-system"
        driver: local


