version: '3.0'

services: 
    cluster-a:
        container_name: cluster-a
        build: 
            context: ./services/data-service-1
            dockerfile: Dockerfile
        ports: 
            - "9200:9200"
        environment: 
            - PORT=9200
        volumes: 
            - cluster-A-volume:/var/lib/data

    cluster-b:
        container_name: cluster-b
        build: 
            context: ./services/data-service-2
            dockerfile: Dockerfile
        ports: 
            - "9300:9300"
        environment: 
            - PORT=9300
        volumes: 
            - cluster-B-volume:/var/lib/data
    
    jupyter:
        container_name: jupyter
        build: 
            context: ./services/jupyter
            dockerfile: Dockerfile
        ports:
            - "8888:8888"
        volumes:
            -  shared-workspace:/opt/workspace 
        networks:
            - spark-net
            
    master:
        build:
            context: .
            dockerfile: services/spark/Dockerfile
        container_name: spark-master
        command: bash -c "start-master.sh && tail -F /dev/null"
        ports:
            - "7077:7077"
            - "8080:8080"
        volumes:
            -  shared-workspace:/opt/workspace 
        networks:
            - spark-net

    worker:
        build:
            context: .
            dockerfile: services/spark/Dockerfile
        environment:
            - SPARK_WORKER_CORES=2
            - SPARK_WORKER_MEMORY=1g
        command: bash -c "start-worker.sh spark://spark-master:7077 && tail -F /dev/null"
        volumes:
            -  shared-workspace:/opt/workspace 
        networks:
            - spark-net

volumes:    
    cluster-A-volume:
    cluster-B-volume:
    shared-workspace:
        name: "hadoop-distributed-file-system"
        driver: local

networks:
    spark-net:



