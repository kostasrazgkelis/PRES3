{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24b832ca",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Spark Commands for the match algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcea257",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Step 1: We need to import our libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b5b7438",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da27c593",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Step 2: We need to start a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a3d0e5f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5b0e84",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Step 3: This is the main algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51d7be08",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def main(hashed_column, noise, csv_a, csv_b):\n",
    "    \n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    myfiles =  {'file': open(csv_a,'rb')}  \n",
    "    requests.post(f\"http://cluster-a:9200//take_data/\", files=myfiles)\n",
    "    request = requests.get(f'http://cluster-a:9200//take_data/{hashed_column}/{noise}')\n",
    "    url_content = request.content\n",
    "    with open(\"/opt/workspace/a_download.csv\", 'wb') as csv_file:\n",
    "        csv_file.write(url_content)\n",
    "\n",
    "    myfiles =  {'file': open(csv_b,'rb')}  \n",
    "    requests.post(f\"http://cluster-b:9300//take_data/\", files=myfiles)\n",
    "    request = requests.get(f'http://cluster-b:9300//take_data/{hashed_column}/{noise}')\n",
    "    url_content = request.content\n",
    "    with open(\"/opt/workspace/b_download.csv\", 'wb') as csv_file:\n",
    "        csv_file.write(url_content)\n",
    "\n",
    "\n",
    "    download_time = datetime.datetime.now() - start\n",
    "\n",
    "    df_1 = spark.read.csv(path=\"/opt/workspace/a_download.csv\", sep=\",\", header=True)\n",
    "    df_2 = spark.read.csv(path=\"/opt/workspace/b_download.csv\", sep=\",\", header=True)\n",
    "\n",
    "    df_1.createOrReplaceTempView(\"a_cluster_data\")\n",
    "    df_2.createOrReplaceTempView(\"b_cluster_data\")\n",
    "    \n",
    "    \n",
    "    start = datetime.datetime.now()\n",
    "    \n",
    "\n",
    "    matched_data = spark.sql(\"SELECT  a.NCID as a_f1, a.first_name as a_f2, a.last_name as a_f3, a.midl_name as a_f4, a.street_name as a_f5, a.res_city_desc as a_f6, \\\n",
    "                                      b.NCID as b_f1, b.first_name as b_f2, b.last_name as b_f3, b.midl_name as b_f4, b.street_name as b_f5, b.res_city_desc as b_f6  \\\n",
    "                              FROM a_cluster_data as a \\\n",
    "                              INNER JOIN b_cluster_data as b \\\n",
    "                              ON a.NCID == b.NCID AND a.first_name == b.first_name AND a.midl_name == b.midl_name AND a.street_name == b.street_name AND a.res_city_desc == b.res_city_desc\")\n",
    "\n",
    "    matched_data.createOrReplaceTempView(\"not_matched_data\")\n",
    "\n",
    "                                 \n",
    "    matched_data.repartition(1).write.mode('overwrite').csv(f\"/opt/workspace/joined_result\", header=True)\n",
    "\n",
    "    joined_time = datetime.datetime.now() - start\n",
    "\n",
    "    matched_data.createOrReplaceTempView(\"matched_data\")\n",
    "    \n",
    "    #The number of the true positives\n",
    "    TP = spark.sql(\"  SELECT *\\\n",
    "                      FROM matched_data \\\n",
    "                      WHERE matched_data.a_f1 == matched_data.b_f1 AND matched_data.a_f2 == matched_data.b_f2 AND matched_data.a_f3 == matched_data.b_f3 AND matched_data.a_f4 == matched_data.b_f4 AND matched_data.a_f5 == matched_data.b_f5 AND matched_data.a_f6 == matched_data.b_f6\")\n",
    "    \n",
    "    total_documents = df_1.count()\n",
    "    total_matches = matched_data.count()\n",
    "    \n",
    "    TP = TP.count()\n",
    "    FP = total_matches - TP\n",
    "    \n",
    "    precision = TP / ( TP + FP ) \n",
    "    \n",
    "    recall    = TP / total_documents\n",
    "    \n",
    "    return pd.DataFrame([[noise, download_time, joined_time , precision, recall ,total_matches, TP, FP ]], columns=['noise', 'download_time', 'joined_time', 'precision', 'recall' ,'total_matches', 'TP', 'FP'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37d6b858",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hashed = 'NCID'\n",
    "for times in [400]:\n",
    "    csv_a = f'{times}K_A.csv'\n",
    "    csv_b = f'{times}K_B.csv'\n",
    "    result = pd.DataFrame(None, columns=['noise', 'download_time', 'joined_time', 'precision', 'recall', 'total_matches', 'TP' , 'FP' ])\n",
    "    for x in range (0,-200,-200):\n",
    "        result = pd.concat([result, main(hashed, x, csv_a, csv_b)], axis=0)\n",
    "\n",
    "    result.to_csv(f'123_{times}.csv', encoding='utf-8', header=True, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208e9f27",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd80a381",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cf15ed",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9783724e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cd4c58",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fcbee6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864c434b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}