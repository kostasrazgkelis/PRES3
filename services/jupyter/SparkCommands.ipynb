{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24b832ca",
   "metadata": {},
   "source": [
    "# Spark Commands for the match algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcea257",
   "metadata": {},
   "source": [
    "Step 1: We need to import our libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b5b7438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import requests\n",
    "import datetime\n",
    "import concurrent.futures\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession \n",
    "import json "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da27c593",
   "metadata": {},
   "source": [
    "Step 2: We need to start a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e3141ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5b0e84",
   "metadata": {},
   "source": [
    "Step 3: This is the main algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51d7be08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(hashed_column, noise, csv_a, csv_b):\n",
    "    \n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    myfiles =  {'file': open(csv_a,'rb')}  \n",
    "    requests.post(f\"http://cluster-a:9200//take_data/\", files=myfiles)\n",
    "    request = requests.get(f'http://cluster-a:9200//take_data/{hashed_column}/{noise}')\n",
    "    url_content = request.content\n",
    "    with open(\"/opt/workspace/a_download.csv\", 'wb') as csv_file:\n",
    "        csv_file.write(url_content)\n",
    "\n",
    "    myfiles =  {'file': open(csv_b,'rb')}  \n",
    "    requests.post(f\"http://cluster-b:9300//take_data/\", files=myfiles)\n",
    "    request = requests.get(f'http://cluster-b:9300//take_data/{hashed_column}/{noise}')\n",
    "    url_content = request.content\n",
    "    with open(\"/opt/workspace/b_download.csv\", 'wb') as csv_file:\n",
    "        csv_file.write(url_content)\n",
    "\n",
    "\n",
    "    download_time = datetime.datetime.now() - start\n",
    "\n",
    "    df_1 = spark.read.csv(path=\"/opt/workspace/a_download.csv\", sep=\",\", header=True)\n",
    "    df_2 = spark.read.csv(path=\"/opt/workspace/b_download.csv\", sep=\",\", header=True)\n",
    "\n",
    "    df_1.createOrReplaceTempView(\"a_cluster_data\")\n",
    "    df_2.createOrReplaceTempView(\"b_cluster_data\")\n",
    "\n",
    "    start = datetime.datetime.now()\n",
    "    \n",
    "    \n",
    "    matched_data = spark.sql(\"  SELECT a.NCID as a_id, a.first_name as a_name, a.last_name as a_surname, b.NCID as b_id, b.first_name as b_name, b.last_name as b_surname   \\\n",
    "                                FROM a_cluster_data as a \\\n",
    "                                INNER JOIN b_cluster_data  as b\\\n",
    "                                ON a.first_name == b.first_name AND a.last_name ==  b.last_name \")  \n",
    "    \n",
    "    matched_data.createOrReplaceTempView(\"not_matched_data\")\n",
    "\n",
    "                                 \n",
    "    #matched_data.repartition(1).write.mode('overwrite').csv(f\"/opt/workspace/joined_result\", header=True)\n",
    "\n",
    "    joined_time = datetime.datetime.now() - start\n",
    "\n",
    "    matched_data.createOrReplaceTempView(\"matched_data\")\n",
    "    \n",
    "    #The number of the true positives\n",
    "    TP = spark.sql(\"  SELECT *\\\n",
    "                      FROM matched_data \\\n",
    "                      WHERE matched_data.a_id == matched_data.b_id and matched_data.a_name == matched_data.b_name and matched_data.a_surname == matched_data.b_surname\")\n",
    "    \n",
    "    total_documents = df_1.count()\n",
    "    total_matches = matched_data.count()\n",
    "    \n",
    "    TP = TP.count()\n",
    "    FP = total_matches - TP\n",
    "    \n",
    "    precision = TP / ( TP + FP ) \n",
    "    \n",
    "    recall    = TP / total_documents\n",
    "    \n",
    "    return pd.DataFrame([[noise, download_time, joined_time , precision, recall ,total_matches, TP, FP ]], columns=['noise', 'download_time', 'joined_time', 'precision', 'recall' ,'total_matches', 'TP', 'FP'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d6b858",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   noise          download_time            joined_time  precision    recall  \\\n",
      "0      0 0 days 00:00:14.629907 0 days 00:00:00.051191   0.137448  0.210380   \n",
      "0     10 0 days 00:00:11.012946 0 days 00:00:00.027296   0.137450  0.191291   \n",
      "0     20 0 days 00:00:11.514408 0 days 00:00:00.035838   0.137525  0.175492   \n",
      "0     30 0 days 00:00:12.820243 0 days 00:00:00.045089   0.137637  0.162185   \n",
      "0     40 0 days 00:00:10.273616 0 days 00:00:00.031899   0.137689  0.150700   \n",
      "..   ...                    ...                    ...        ...       ...   \n",
      "0    950 0 days 00:01:40.218945 0 days 00:00:00.071240   0.278972  0.049461   \n",
      "0    960 0 days 00:01:12.501140 0 days 00:00:00.022133   0.282023  0.049820   \n",
      "0    970 0 days 00:01:26.401098 0 days 00:00:00.170046   0.283330  0.049682   \n",
      "0    980 0 days 00:01:18.333027 0 days 00:00:00.519670   0.286270  0.049857   \n",
      "0    990 0 days 00:01:30.780653 0 days 00:00:00.036341   0.288286  0.050011   \n",
      "\n",
      "   total_matches     TP      FP  \n",
      "0         153062  21038  132024  \n",
      "0         153088  21042  132046  \n",
      "0         153129  21059  132070  \n",
      "0         153186  21084  132102  \n",
      "0         153229  21098  132131  \n",
      "..           ...    ...     ...  \n",
      "0         186162  51934  134228  \n",
      "0         187251  52809  134442  \n",
      "0         187626  53160  134466  \n",
      "0         188095  53846  134249  \n",
      "0         189090  54512  134578  \n",
      "\n",
      "[100 rows x 8 columns]\n",
      "   noise          download_time            joined_time  precision    recall  \\\n",
      "0      0 0 days 00:00:11.979110 0 days 00:00:00.015792   0.137448  0.210380   \n",
      "0     10 0 days 00:00:07.434429 0 days 00:00:00.034847   0.137446  0.191282   \n",
      "0     20 0 days 00:00:12.965869 0 days 00:00:00.038901   0.137468  0.175417   \n",
      "0     30 0 days 00:00:11.958887 0 days 00:00:00.009321   0.137510  0.162031   \n",
      "0     40 0 days 00:00:10.789172 0 days 00:00:00.048255   0.137725  0.150721   \n",
      "..   ...                    ...                    ...        ...       ...   \n",
      "0    950 0 days 00:01:59.450848 0 days 00:00:00.112222   0.279507  0.049616   \n",
      "0    960 0 days 00:02:04.729744 0 days 00:00:00.118212   0.282485  0.049927   \n",
      "0    970 0 days 00:02:05.655145 0 days 00:00:00.073529   0.284066  0.049875   \n",
      "0    980 0 days 00:02:03.774170 0 days 00:00:00.096279   0.286776  0.050030   \n",
      "0    990 0 days 00:01:39.183304 0 days 00:00:00.069964   0.287541  0.049732   \n",
      "\n",
      "   total_matches     TP      FP  \n",
      "0         153062  21038  132024  \n",
      "0         153086  21041  132045  \n",
      "0         153127  21050  132077  \n",
      "0         153182  21064  132118  \n",
      "0         153211  21101  132110  \n",
      "..           ...    ...     ...  \n",
      "0         186389  52097  134292  \n",
      "0         187348  52923  134425  \n",
      "0         187865  53366  134499  \n",
      "0         188412  54032  134380  \n",
      "0         188523  54208  134315  \n",
      "\n",
      "[100 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "hashed = 'NCID'\n",
    "for times in [100,200,400]:\n",
    "    csv_a = f'book_chapter_table_25p_{times}k_A.csv'\n",
    "    csv_b = f'book_chapter_table_25p_{times}k_1_B.csv'\n",
    "    result = pd.DataFrame(None, columns=['noise', 'download_time', 'joined_time', 'precision', 'recall', 'total_matches', 'TP' , 'FP' ])\n",
    "    for x in range (0,1000,10):\n",
    "        result = pd.concat([result, main(hashed, x, csv_a, csv_b)], axis=0)\n",
    "\n",
    "    result.to_csv(f'final_data_{times}.csv', encoding='utf-8', header=True, index=False)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208e9f27",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd80a381",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cf15ed",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9783724e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cd4c58",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fcbee6",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
